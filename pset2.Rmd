---
title: "Math for Data Science: Problem Set 2"
output: pdf_document
author: "Group: GROUP_MEMBERS_NAMES" 
date: "2025-11-11"
header-includes:
  - \usepackage{comment}
  - \newcommand{\BetaDist}{\mathrm{Beta}}
  - \newcommand{\Binom}{\mathrm{Binomial}}
  - \newcommand{\E}{\mathbb{E}}
  - \newcommand{\Prob}{\mathbb{P}}
params:
  soln: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due Date:** Thursday, November 20 by the end of the day. (The Moodle submission link will become inactive at midnight of November 21.)

**Instructions:** Please submit one solution set per group and include your group members' names at the top. This time, please write your solutions within this Rmd file, under the relevant question. Please submit the knitted output as a pdf. Make sure to show all code you used to arrive at the answer. However, please provide a brief, clear answer to every question rather than making us infer it from your output, and please avoid printing unnecessary output.

## 1. The Law of Large Numbers and the Central Limit Theorem

You are an urban planner interested in finding out how many people enter and leave the city using personal vehicles every day. (You're not interested in the number of *cars*; you're interested in the number of *people* who use cars to get to work.) To do this, you decide to collect data from a few different points around the city on how many people there are per car. You already have reliable satellite data on the number of cars that come into the city, so if you get a good estimate of people per car you'll be in good shape.

Collecting data on people per car is costly and you'd love to minimize how many data points you have to collect. However, you're also familiar with the Law of Large Numbers and know that the sample mean converges to the true mean as the sample size $n$ grows large.

a.  Let's illustrate this with a small simulation. Suppose the number of people in a car is distributed Poisson with a rate of $\lambda=2$ people per car.[^1] Construct 500 samples from this distribution, with the first sample having $n=1$ cars, the second $n=2$ cars, and so on. Compute the average number of people per car in each sample. Plot this on the y-axis against the sample size on the x-axis and run a horizontal blue line through the true mean. Comment on what you see.

[^1]: I should have mentioned that you're an urban planner in San Francisco, where it's rare but possible to have 0 people in a car.

```{r, echo = TRUE, include = TRUE}

# Set parameters
set.seed(666)
lambda <- 2
max_n <- 500

# Generate sample means for each n
sample_means <- sapply(1:max_n, function(n) {
  mean(rpois(n, lambda))
})

# Plot results
plot(1:max_n, sample_means, type = "l",
     xlab = "Sample size (n)",
     ylab = "Average people per car",
     main = "Law of Large Numbers: Sample Mean of Poisson(lambda = 2)")
abline(h = lambda, col = "blue", lwd = 2, lty = 2)

```

b.  You collect data on 100 cars and compute the average number of people per car in this sample. Use the Central Limit Theorem to write down the approximate distribution of this quantity.

$$

We observe the number of people per car for a sample of \(n = 100\) cars. Let
\[
X_i = \text{number of people in car } i,\quad i = 1,\dots,100,
\]
and suppose that
\[
X_i \stackrel{\text{iid}}{\sim} \text{Poisson}(\lambda = 2).
\]

For a Poisson\((\lambda)\) random variable, we have
\[
\mathbb{E}[X_i] = \lambda = 2
\quad\text{and}\quad
\mathrm{Var}(X_i) = \lambda = 2.
\]

The sample mean number of people per car is
\[
\bar X_{100} = \frac{1}{100}\sum_{i=1}^{100} X_i.
\]

By the Central Limit Theorem, the sampling distribution of the sample mean is approximately Normal:
\[
\bar X_n \approx \mathcal{N}\!\left(\mathbb{E}[X_i],\ \frac{\mathrm{Var}(X_i)}{n}\right).
\]

Substituting \(\mathbb{E}[X_i] = 2\), \(\mathrm{Var}(X_i) = 2\), and \(n = 100\), we obtain
\[
\bar X_{100} \approx \mathcal{N}\!\left(2,\ \frac{2}{100}\right)
    = \mathcal{N}(2,\, 0.02).
\]

Equivalently, the standard error of the sample mean is
\[
\mathrm{SE}(\bar X_{100}) 
    = \sqrt{\frac{2}{100}} 
    = \sqrt{0.02}
    \approx 0.1414.
\]

Thus, the approximate distribution of the average number of people per car in a sample of 100 cars is
\[
\boxed{\bar X_{100} \approx \mathcal{N}(2,\, 0.02).}
\]


$$

c.  Let's examine this distribution more closely. Generate 10,000 replicates of the sample mean with $n=100$ and plot a histogram.[^2] Are you convinced that the Normal approximation you found in the previous question is good enough? Compare this to $n=1$, $n=5$, and $n=30$, generating a histogram for each. (We're aiming to recreate the second row of Figure 10.5 from Slide 47 of Lecture 4.) Comment on what you observe.

[^2]: Try using the `replicate` function rather than a loop, as this will speed things up considerably.

```{r, echo = TRUE, include = TRUE}
library(ggplot2)

# Setup
set.seed(666)
lambda <- 2
reps   <- 10000
Ns     <- c(1, 5, 30, 100)

# Simulate sample means using replicate()
sim_mat <- sapply(Ns, function(n) replicate(reps, mean(rpois(n, lambda))))
colnames(sim_mat) <- paste0("n=", Ns)

# Tidy for plotting
df <- data.frame(
  mean = as.vector(sim_mat),
  n    = factor(rep(Ns, each = reps), levels = Ns)
)

# Normal curves to overlay
xgrid <- seq(0, 5, by = 0.01)
norm_curves <- do.call(rbind, lapply(Ns, function(n) {
  data.frame(
    x = xgrid,
    y = dnorm(xgrid, mean = lambda, sd = sqrt(lambda / n)),
    n = factor(n, levels = Ns)
  )
}))

ggplot(df, aes(x = mean)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = "lightgray", color = "black") +
  geom_line(data = norm_curves, aes(x = x, y = y), linewidth = 1, color = "blue") +
  facet_wrap(~ n, nrow = 2) +
  coord_cartesian(xlim = c(0, 5)) +
  labs(title = "Sampling distribution of the sample mean (Poisson lambda = 2)",
       x = "Sample mean", y = "Density") +
  theme_minimal()

```

d.  Suppose the city government will enact measures to regulate the number of people allowed per car during rush hour if they think the mean is below 1.7 people per car. Using the Normal approximation from part (b) above, find the probability that you get a mean of 1.7 or less in your sample of 100, even though the true mean is 2. (Please give the theoretical answer, not a simulation. You can use `R` as a calculator.) What should you do to ensure that this probability stays below 1%?

```{r, echo = TRUE, include = TRUE}

# X_i ~ Poisson(lambda = 2)
# Under the CLT, the sample mean is approximately Normal:
# Xbar_n ~ Normal(mean=lambda, variance = lambda / n)
# For n = 100:
lambda <- 2
n <- 100
mean_true <- lambda
sd_true <- sqrt(lambda / n)

# The city will act if the sample mean equal or less to 1.7
threshold <- 1.7

# Compute the probability of observing a mean equal or less 1.7
p_value <- pnorm(threshold, mean = mean_true, sd = sd_true)
p_value
# app. 0.0169 or 1.7%

# Interpretation output:
cat(
  "Using the CLT, we approximate the sampling distribution\n",
  "of the sample mean as Normal(mu = 2, sigma^2 = 2/100).\n",
  "The probability that the observed mean is 1.7 or lower,\n",
  "even though the true mean is 2, equals\n",
  "Phi((1.7 - 2) / sqrt(2/100)) = 0.0169,\n",
  "which is about 1.7%.\n",
  "Thus, there is a 1.7% chance of falsely concluding\n",
  "the mean is below 1.7.\n"
)

# To ensure this false-alarm probability is below 1%, we solve for n:
target_alpha <- 0.01
z_alpha <- qnorm(1 - target_alpha)
n_required <- ceiling(lambda / ((0.3 / z_alpha)^2))
n_required

# Interpretation output:
cat(
  "To reduce this probability below 1%, we need at least n =",
  n_required,
  "observations.\n"
)

```

## 2. Maximum Likelihood

Bangladesh, home to 163 million people, is the world's most populous delta region; one-fourth of the country's land mass is only seven feet above sea level.\footnote{\url{https://www.nrdc.org/stories/bangladesh-country-underwater-culture-move}} Although the communities in Bangladesh's low-lying coastal regions have always been vulnerable to catastrophic flooding events, this seems to be happening with growing frequency. Is climate change increasing the occurrence of flooding in Bangladesh?

We often use the Poisson distribution to model (rare) climate events such as earthquakes and hurricanes. So let $X_t$ be the number of major floods in Bangladesh in time period $t$, and let $X_t$ be distributed: $$ X_t \sim \text{Poisson}(\lambda) $$

a.  We observe the following number of floods in Bangladesh per five-year period for the first quarter of the 21st century: $$
    \left[    \begin{array}{cc}
        1  & 2000-2004 \\
        3  & 2005-2009 \\
        1  & 2010-2014 \\
        2  & 2015-2019 \\
        0  & 2020-2024
     \end{array} \right] 
    $$ Please write down the likelihood of this series of events for some unknown $\lambda$, assuming the floods in each period are independent and identically distributed.

The probability function for a Poisson distribution is:

$P(X=x) = \frac{e^{-\lambda}\lambda^x}{x!}$

Therefore, the likelihood function for the series of events is:

$$
L(\lambda) = \prod_{t=1}^{n} P(X_t = x_t)
= \prod_{t=1}^{n} \left( \frac{e^{-\lambda} \lambda^{x_t}}{x_t!} \right).$$

Adding in the values for x:

$$
\begin{aligned}
L(\lambda) = \prod_{t=1}^{5} \left(\frac{e^{-\lambda}\lambda^{x_t}}{x_t!}\right) 
&= \left(\frac{e^{-\lambda}\lambda^{1}}{1!}\right)\left(\frac{e^{-\lambda}\lambda^{3}}{3!}\right)\left(\frac{e^{-\lambda}\lambda^{1}}{1!}\right)\left(\frac{e^{-\lambda}\lambda^{2}}{2!}\right)\left(\frac{e^{-\lambda}\lambda^{0}}{0!}\right) \\
&= \left(\frac{e^{-\lambda}\lambda}{1}\right)\left(\frac{e^{-\lambda}\lambda^{3}}{6}\right)\left(\frac{e^{-\lambda}\lambda}{1}\right)\left(\frac{e^{-\lambda}\lambda^{2}}{2}\right)\left(\frac{e^{-\lambda}}{1}\right) \\
&= \left(\frac{e^{-5\lambda}\lambda^{7}}{12}\right)
\end{aligned}
$$

b.  Take the log of the likelihood you wrote down in part (a). Show all steps.

$$
\begin{aligned}
\ell(\lambda)
&= \log L(\lambda) \\
&= \log\left( \prod_{t=1}^n \frac{e^{-\lambda}\lambda^{x_t}}{x_t!} \right) \\
&= \sum_{t=1}^n \log\left( \frac{e^{-\lambda}\lambda^{x_t}}{x_t!} \right) \\
&= \sum_{t=1}^n ( x_t \log \lambda -\lambda - \log(x_t!) ) \\
&= \log \lambda\sum_{t=1}^nx_t - n \lambda - \sum_{t=1}^n\log(x_t!)
\end{aligned}
$$

Adding in the values for x:

$$
\begin{aligned}
\ell(\lambda)
&= \log \lambda\sum_{t=1}^5x_t - 5 \lambda - \sum_{t=1}^5\log(x_t!) \\
&= \log \lambda(1+3+1+2+0) - 5 \lambda - (\log(1) + \log(6) + \log(1) + \log(2) + \log(1)) \\
&= 7\log \lambda - 5 \lambda - 2.48
\end{aligned}
$$

c.  Maximize the log-likelihood from part (b) to derive an MLE estimator for $\lambda$. Show all steps.

In order to maximize the log-likelihood function we must set $\frac{d\ell(\lambda)}{d\lambda} = 0$

Where $\ell(\lambda) = \log \lambda\sum_{t=1}^nx_t - n \lambda - \sum_{t=1}^n\log(x_t!)$

$$
\begin{aligned}
\frac{d\ell(\lambda)}{d\lambda}
= -n + \frac{1}{\lambda}\sum_{t=1}^n x_t
= 0 \\[8pt]
\frac{1}{\lambda}\sum_{t=1}^n x_t = n \\[6pt]
\frac{1}{\lambda} = \frac{n}{\sum_{t=1}^n x_t} \\[6pt]
\lambda = \frac{\sum_{i=1}^n x_t}{n}.
\end{aligned}
$$

Adding in the values for x:

$$
\lambda = \frac{\sum_{i=1}^n x_t}{n} = \frac{1+3+1+2+0}{7} = \frac{7}{5} = 1.4
$$

The maximum likelihood estimator is $\hat{\lambda} = \frac{\sum_{t=1}^n x_t}{n} = 1.4$.

d.  Interpret the $\hat{\lambda}$ you found in part (c) in your own words. What is this quantity conceptually, and how do you get it from the data?

$\hat{\lambda}$ is the sample mean of the counts and (for the Poisson model) the MLE of the Poisson rate: it estimates the average number of major floods per five-year period.

*Because each observation represents a five-year period, the estimated rate of 1.4 floods per period corresponds to roughly 0.28 major floods per year.*

e.  Show that you found the MLE by plotting the log likelihood on the y-axis against a series of candidate values for $\lambda$ ranging from 0 to 4 on the x-axis.

```{r, echo = TRUE, include = TRUE}
# Setup
set.seed(666)
flood_count <- c(1, 3, 1, 2, 0)

# Log likelihood function
likelihood <- function(x, l){
  lik <- sum(x * log(l) - l - log(factorial(x)))
  return(lik)
}

likelihood(flood_count, l = 1.4) # -7.129601

# Plot setup
lik_df <- data.frame(lambda = seq(0.001, 4, length.out = 100), # 0.001 to avoid NaN
                     likelihood = NA)
lik_df$likelihood <- sapply(X = lik_df$lambda, FUN = likelihood, x = flood_count)

# Log likelihood plot
library(ggplot2)
ggplot(lik_df, aes(x = lambda, y = likelihood)) + 
  geom_point() + 
  geom_line() + 
  geom_hline(yintercept = max(lik_df$likelihood), color = "red") + 
  geom_vline(xintercept = 1.4, color = "red") + 
  xlab("Lambda") + 
  ylab("Likelihood") + 
  theme_minimal()

```

## 3. Bayesian Analysis

You monitor the presence of a blue--green algae species across freshwater sites. In a sample of $n=274$ sites, you observe algae present at $y=44$ sites. Let $\theta \in (0,1)$ denote the true probability that a randomly selected site has detectable algae.[^3]

[^3]: This question is adapted from <https://avehtari.github.io/BDA_course_Aalto/assignments/assignment2.html>.

a.  Assume: 
$$
    y \sim \Binom(n,\theta), \qquad \theta \sim \BetaDist(\alpha,\beta).
    $$ 
Take as a baseline prior $\alpha=2,\ \beta=10$. Using Beta--Binomial conjugacy, write down the posterior $p(\theta\mid y,n)$ and identify its parameters.

b.  Give the expression (in terms of $\alpha,\beta,y,n$) for the posterior mean of $\theta$.

c. Alternatively, we may consider using the priors below:
\[
\mathrm{Beta}(1,1)\quad\text{(uniform)}
\]
\[
\mathrm{Beta}(0.5,0.5)\quad\text{(Jeffreys-type weak prior)}
\]
\[
\mathrm{Beta}(100,2)\quad\text{(strongly informative, favoring large $\theta$)}.
\]

    Please plot, on a common $\theta \in [0,1]$ axis, the posterior densities for the four priors (the baseline prior in part (a) and the alternative priors above).

```{r, echo = TRUE, include = TRUE}

```

d.  In a few sentences, interpret how prior shape and strength influence the posterior relative to the data. Which prior(s) seem the most defensible in this context? If you were interested in monitoring algae presence, what would be your takeaway from this analysis?
